{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06f4599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianitalopez/Documents/GitHub/Corridos/.p3venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import lxml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2dc62",
   "metadata": {},
   "source": [
    "In this section, one will see how to web scrap data from Wikipedia. This was done to help create a pipeline to help with identifing Ghost Towns, Unicorporated areas (ie. Texas Colonias), and  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04086688",
   "metadata": {},
   "source": [
    "## Web Scrap and Extract List of municipalities, ghost town, and unicorporated in Texas from Wiki \n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_municipalities_in_Texas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88779cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def heading_extract(a,b):\n",
    "    url= a \n",
    "    file = f\"../corridos/{b}.txt\"\n",
    "    s = requests.get(url)\n",
    "    soup=BeautifulSoup(s.text, \"lxml\")#to tell the program to read the site as HTML (lxml)\n",
    "    #find a specific item from the website to make scrapping easier. \n",
    "    heading = soup.find(\"h1\",{\"id\": 'firstHeading'})\n",
    "    with open(file,\"a+\", encoding='utf-8') as f:\n",
    "            f.write(heading.get_text()+\"\\n\") #type:ignore\n",
    "'''\n",
    "#heading_extract()#check file explore\n",
    "#ensure the text file is there and inputting data from the web.\n",
    "\n",
    "def table_extraction(a,b):\n",
    "    url= a \n",
    "    file = f\"../corridos/{b}.txt\"\n",
    "    s = requests.get(url)\n",
    "    soup= BeautifulSoup(s.text, \"lxml\") #to tell the program to read the site as HTML (lxml)\n",
    "    table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "    with open (file, \"w\", encoding=\"utf-8\")as f: \n",
    "        #rows=list()\n",
    "        #headerString  = \"\"\n",
    "        dataString = \"\"\n",
    "        for row in table.find_all(\"tr\"):   \n",
    "            #for th in row.find_all(\"th\"): \n",
    "                #headerString  = headerString+th.get_text(strip=True)+\"|\"\n",
    "                #print(headerString)\n",
    "                #ending a line break to organize the txt file\n",
    "            for td in row.find_all(\"td\"):\n",
    "                dataString = dataString+td.get_text(strip=True)+\"|\"\n",
    "            dataString=dataString + \"\\n\"\n",
    "        #f.write(headerString)       \n",
    "        f.write(dataString)\n",
    "\n",
    "def do_all(a, b):\n",
    "    #heading_extract(a,b)\n",
    "    table_extraction(a,b)\n",
    "\n",
    "do_all(\"https://en.wikipedia.org/wiki/List_of_municipalities_in_Texas\",\"List of municipalities in Texas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e3c01",
   "metadata": {},
   "source": [
    "List of unincorporated communities in Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_extraction(a,b):\n",
    "    url= a \n",
    "    file = f\"list of texas places/{b}.txt\" \n",
    "    s = requests.get(url)\n",
    "    soup= BeautifulSoup(s.text, \"html.parser\") #to tell the program to read the site as HTML (lxml)\n",
    "    table = soup.find(\"table\", class_=\"wikitable sortable mw-collapsible\")\n",
    "    with open (file, \"w\", encoding=\"utf-8\")as f: \n",
    "        #rows=list()\n",
    "        #headerString  = \"\"\n",
    "        dataString = \"\"\n",
    "        for row in table.find_all(\"tr\"):   \n",
    "            #for th in row.find_all(\"th\"): \n",
    "                #headerString  = headerString+th.get_text(strip=True)+\"|\"\n",
    "                #print(headerString)\n",
    "                #ending a line break to organize the txt file\n",
    "            for td in row.find_all(\"td\"):\n",
    "                dataString = dataString+td.get_text(strip=True)+\"|\"\n",
    "            dataString=dataString + \"\\n\"\n",
    "        #f.write(headerString)       \n",
    "        f.write(dataString)\n",
    "\n",
    "def do_all(a, b):\n",
    "    table_extraction(a,b)\n",
    "\n",
    "do_all(\"https://en.wikipedia.org/wiki/List_of_unincorporated_communities_in_Texas\", \"List of unincorporated communities in Texas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d034b",
   "metadata": {},
   "source": [
    "List of Ghost Towns in Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01944a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_extraction(a,b):\n",
    "    url= a \n",
    "    file = f\"list of texas places/{b}.txt\" \n",
    "    s = requests.get(url)\n",
    "    soup= BeautifulSoup(s.text, \"html.parser\") #to tell the program to read the site as HTML (lxml)\n",
    "    table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "    with open (file, \"w\", encoding=\"utf-8\")as f: \n",
    "        #rows=list()\n",
    "        #headerString  = \"\"\n",
    "        dataString = \"\"\n",
    "        for row in table.find_all(\"tr\"):   \n",
    "            #for th in row.find_all(\"th\"): \n",
    "                #headerString  = headerString+th.get_text(strip=True)+\"|\"\n",
    "                #print(headerString)\n",
    "                #ending a line break to organize the txt file\n",
    "            for td in row.find_all(\"td\"):\n",
    "                dataString = dataString+td.get_text(strip=True)+\"|\"\n",
    "            dataString=dataString + \"\\n\"\n",
    "        #f.write(headerString)       \n",
    "        f.write(dataString)\n",
    "\n",
    "def do_all(a, b):\n",
    "    table_extraction(a,b)\n",
    "\n",
    "do_all(\"https://en.wikipedia.org/wiki/List_of_ghost_towns_in_Texas\", \"List of ghost towns in Texas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f38071",
   "metadata": {},
   "source": [
    "List of Municipalities in Mexico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7f63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of municipalities in mexico \n",
    "\n",
    "'''\n",
    "def heading_extract(a,b):\n",
    "    url= a \n",
    "    file = f\"../corridos/{b}.txt\"\n",
    "    s = requests.get(url)\n",
    "    soup=BeautifulSoup(s.text, \"lxml\")#to tell the program to read the site as HTML (lxml)\n",
    "    #find a specific item from the website to make scrapping easier. \n",
    "    heading = soup.find(\"h1\",{\"id\": 'firstHeading'})\n",
    "    with open(file,\"a+\", encoding='utf-8') as f:\n",
    "            f.write(heading.get_text()+\"\\n\") #type:ignore\n",
    "              \n",
    "#heading_extract()#check file explore\n",
    "#ensure the text file is there and inputting data from the web.\n",
    "'''\n",
    "def table_extraction(a,b):\n",
    "    url= a \n",
    "    file = f\"../corridos/{b}.txt\"\n",
    "    s = requests.get(url)\n",
    "    soup= BeautifulSoup(s.text, \"lxml\") #to tell the program to read the site as HTML (lxml)\n",
    "    table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "    with open (file, \"w\", encoding=\"utf-8\")as f: \n",
    "        #rows=list()\n",
    "        #headerString  = \"\"\n",
    "        dataString = \"\"\n",
    "        for row in table.find_all(\"tr\"):   \n",
    "            #for th in row.find_all(\"th\"): \n",
    "                #headerString  = headerString+th.get_text(strip=True)+\"|\"\n",
    "                #print(headerString)\n",
    "                #ending a line break to organize the txt file\n",
    "            for td in row.find_all(\"td\"):\n",
    "                dataString = dataString+td.get_text(strip=True)+\"|\"\n",
    "            dataString=dataString + \"\\n\"\n",
    "        #f.write(headerString)       \n",
    "        f.write(dataString)\n",
    "\n",
    "def do_all(a, b):\n",
    "    #heading_extract(a,b)\n",
    "    table_extraction(a,b)\n",
    "\n",
    "do_all(\"https://en.wikipedia.org/wiki/List_of_municipalities_in_Mexico_by_population\",\"List of municipalities in Mexico\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Greg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c33b8046986a03aa44ef4ea500a2630dcd79ff60fec54402ff1309c4dcbd3e63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
