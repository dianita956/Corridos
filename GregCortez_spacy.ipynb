{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a73489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import geopandas as geopd\n",
    "import pandas as pd\n",
    "import os\n",
    "import fiona \n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "from shapely.wkt import loads\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5540771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865058ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb12e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install geograpy3\n",
    "import geograpy\n",
    "from geograpy import extraction\n",
    "from geograpy import places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d32cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b55db5",
   "metadata": {},
   "source": [
    "El Corrido de Gregorio Cortez \n",
    "by Diane Lopez\n",
    "Septemeber 8 2022\n",
    "text analysis sentiment and gis project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading text file and creating a doc object by processing a string of text with the nlp object \n",
    "# Replace line breaks with spaces\n",
    "\n",
    "#reading text file\n",
    "gregText = open(\"corrido corpus\\gregoriocortez_es_corrido.txt\", encoding=\"utf-8\").read()\n",
    "# Replace line breaks with spaces\n",
    "text_linebreaks = gregText.replace('\\n', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_md \n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(text_linebreaks)\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "   print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b759260b",
   "metadata": {},
   "source": [
    "NER Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER with Long Texts or Many Texts\n",
    "import math\n",
    "number_of_chunks = 80\n",
    "\n",
    "chunk_size = math.ceil(len(text) / number_of_chunks)\n",
    "\n",
    "text_chunks = []\n",
    "\n",
    "for number in range(0, len(text), chunk_size):\n",
    "    text_chunk = text[number:number+chunk_size]\n",
    "    text_chunks.append(text_chunk)\n",
    "\n",
    "chunked_documents = list(nlp.pipe(text_chunks))\n",
    "chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea792333",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "for text in chunked_documents:\n",
    "    for named_entity in document.ents:\n",
    "        if named_entity.label_ == \"LOC\":\n",
    "            places.append(named_entity.text)\n",
    "\n",
    "places_tally = Counter(places)\n",
    "\n",
    "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get NER in Context\n",
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def get_ner_in_context(keyword, document, desired_ner_labels= False):\n",
    "    \n",
    "    if desired_ner_labels != False:\n",
    "        desired_ner_labels = desired_ner_labels\n",
    "    else:\n",
    "        desired_ner_labels = ['PER', 'ORG', 'LOC']  \n",
    "        \n",
    "    #Iterate through all the sentences in the document and pull out the text of each sentence\n",
    "    for sentence in document.sentences:\n",
    "        #process each sentence\n",
    "        sentence_doc = nlp(sentence.text)\n",
    "        for named_entity in sentence_doc.ents:\n",
    "            #Check to see if the keyword is in the sentence (and ignore capitalization by making both lowercase)\n",
    "            if keyword.lower() in named_entity.text.lower()  and named_entity.label_ in desired_ner_labels:\n",
    "                #Use the regex library to replace linebreaks and to make the keyword bolded, again ignoring capitalization\n",
    "                #sentence_text = sentence.text\n",
    "            \n",
    "                sentence_text = re.sub('\\n', ' ', sentence.text)\n",
    "                sentence_text = re.sub(f\"{named_entity.text}\", f\"**{named_entity.text}**\", sentence_text, flags=re.IGNORECASE)\n",
    "\n",
    "                display(Markdown('---'))\n",
    "                display(Markdown(f\"**{named_entity.label_}**\"))\n",
    "                display(Markdown(sentence_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in chunked_documents:\n",
    "    get_ner_in_context('Laredo', document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60262c4",
   "metadata": {},
   "source": [
    "Named Entity Recongnition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#People\n",
    "people = []\n",
    "\n",
    "for document in chunked_documents:\n",
    "    for named_entity in document.ents:\n",
    "        if named_entity.label_ == \"PER\":\n",
    "            people.append(named_entity.text)\n",
    "\n",
    "people_tally = Counter(people)\n",
    "\n",
    "gregPpl_df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
    "gregPpl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fba158",
   "metadata": {},
   "source": [
    "Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236442f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5e53f",
   "metadata": {},
   "source": [
    "Keyword Extration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_with_keyword(keyword, doc):\n",
    "    \n",
    "    #Iterate through all the sentences in the document and pull out the text of each sentence\n",
    "    for sentence in doc.sents:\n",
    "        sentence = sentence.text\n",
    "        \n",
    "        #Check to see if the keyword is in the sentence (and ignore capitalization by making both lowercase)\n",
    "        if keyword.lower() in sentence.lower():\n",
    "            \n",
    "            #Use the regex library to replace linebreaks and to make the keyword bolded, again ignoring capitalization\n",
    "            sentence = re.sub('\\n', ' ', sentence)\n",
    "            sentence = re.sub(f\"{keyword}\", f\"**{keyword}**\", sentence, flags=re.IGNORECASE)\n",
    "\n",
    "            display(Markdown(sentence))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sentences_with_keyword(keyword=\"Laredo\", doc=doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a list of tokens and POS labels from document if the token is a word \n",
    "tokens_and_labels = [(token.text, token.pos_) for token in doc if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to get all two-word combinations\n",
    "def get_bigrams(word_list, number_consecutive_words=2):\n",
    "    \n",
    "    ngrams = []\n",
    "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
    "    \n",
    "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
    "    for word_index in range(adj_length_of_word_list):\n",
    "        \n",
    "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
    "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
    "        \n",
    "        #Append this word combo to the master list \"ngrams\"\n",
    "        ngrams.append(ngram)\n",
    "        \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc99fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = get_bigrams(tokens_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams[5:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
    "    \n",
    "    neighbor_words = []\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        \n",
    "        #Extract just the lowercased words (not the labels) for each bigram\n",
    "        words = [word.lower() for word, label in bigram]        \n",
    "        \n",
    "        #Check to see if keyword is in the bigram\n",
    "        if keyword in words:\n",
    "            \n",
    "            for word, label in bigram:\n",
    "                \n",
    "                #Now focus on the neighbor word, not the keyword\n",
    "                if word.lower() != keyword:\n",
    "                    #If the neighbor word matches the right pos_label, append it to the master list\n",
    "                    if label == pos_label or pos_label == None:\n",
    "                        neighbor_words.append(word.lower())\n",
    "    \n",
    "    return Counter(neighbor_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_neighbor_words(\"Cortez\", bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "greg_df = pd.read_fwf('log.csv')\n",
    "greg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user mordecai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052bf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mordecai\n",
    "from mordecai import Geopareser \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('GregCortez')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a27f88ce5c954a07fd947c99c0c9c106cdd63656ed15b6da87f708bf4b75587"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
