# -*- coding: utf-8 -*-
"""SpacyDocBin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17aIHkCu4SVyANcyBAUBFdvM7o-JDAKzV

#downloading and importing spacy and spanish model medium
"""

!python -m spacy download es_core_news_md
import spacy

nlp = spacy.load('es_core_news_md')

import re

"""#import text

"""

with open('la toma de Matamoros.txt', 'r', encoding='utf-8') as c:
  text = c.read()
  text = re.sub('\n(.)', r'\1', text)
  print(text)

with open('el general Cortina.txt', 'r', encoding='utf-8') as c2:
  text2 = c2.read()
  text2 = re.sub('\n(.)', r'\1', text2)
  print(text2)

"""#creating a doc container"""

doc = nlp(text)
print(doc)

for token in text[:10]:
  print(token)

for token in doc [:len(doc)]:
  print(token)

for token in text.split()[:10]:
    print (token)

words = text.split()[:10]

i=5
for token in doc[i:8]:
  print(f"SpaCy Token {i}: \n{token}\nWord Split{i}:\n{words[i]}\n\n")
  i=i+1

"""#creating a doc2 container"""

doc2 = nlp(text2)
print(doc2)

for token2 in text2[:10]:
  print(token2)

for token2 in doc2[:17]:
  print(token2)

"""#sentence boundary detection(sb)

"""

for sent in doc.sents:
  print(sent)

#Let’s move forward with just one of these sentences. Let’s try and grab index 0 in this attribute.
sentence1 = list(doc.sents)[0]
print (sentence1)

"""#token attributes

from Intro to spacy 3 by Matt...(cita later)
The token object contains a lot of different attributes that are VITAL do performing NLP in spaCy. We will be working with a few of them, such as:

.text

.head

.left_edge

.right_edge

.ent_type_

.iob_

.lemma_

.morph

.pos_

.dep_

.lang_

I will briefly describe these here and show you how to grab each one and what they look like. We will be exploring each of these attributes more deeply in this chapter and future chapters. To demonstrate each of these attributes, we will use one token, “States” which is part of a sequence of tokens that make up “The United States of America”
"""

sentence1

token2 = sentence1[13]
print (token2)

#text
token2.text

#head
token2.head

#left edge
token2.left_edge

#right edge
token2.right_edge

#entity type
token2.ent_type

token2.ent_type_

#ent IOB; '''IOB code of named entity tag. “B” means the token begins an entity,
#“I” means it is inside an entity,
#“O” means it is outside an entity, and "" means no entity tag is set.'''
token2.ent_iob_

#lemma
token2.lemma_

sentence1[13].lemma_

#morphological analysis
sentence1[13].morph

#part of speech
token2.pos_

#syntactic dependency
token2.dep_

#language
token2.lang_

"""#Part of Speech tagging
In the field of computational linguistics, understanding parts-of-speech is essential. SpaCy offers an easy way to parse a text and identify its parts of speech. Below, we will iterate across each token (word or punctuation) in the text and identify its part of speech.
"""

for token in sentence1:
    print (token.text, token.pos_, token.dep_)

import spacy
from spacy import displacy
displacy.render(sentence1, style="dep", jupyter=True, options={})

displacy.render(doc, style="ent", jupyter=True)

"""#named entity recogniton"""

for ent in doc.ents:
    print (ent.text, ent.label_)

displacy.render(doc, style="ent", jupyter=True)

"""#Word Vectors
REsources
https://pypi.org/project/PyDictionary/
"""

!pip install PyDictionary
from PyDictionary import PyDictionary

dictionary=PyDictionary()

words  = ["cantar", "vida"]
for word in words:
    syns = dictionary.synonym(word)
    print (f"{word}: {syns[0:5]}\n")

sentence1[0].vector

""" Why use Word Vectors?

Once a word vector model is trained, we can do similarity matches very quickly and very reliably. Let’s explore some vectors from our medium sized model. Let’s specifically try and find the words most closely related to the word dog.[cita]
"""

import numpy as np
#https://stackoverflow.com/questions/54717449/mapping-word-vector-to-the-most-similar-closest-word-using-spacy
your_word = "atención"

ms = nlp.vocab.vectors.most_similar(
    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10)
words = [nlp.vocab.strings[w] for w in ms[0][0]]
distances = ms[2]
print(words)

"""#doc similarity"""

#doc similarity
# Similarity of two documents
print(doc, "<->", doc2, doc.similarity(doc2))

"""#word similarity"""

#similarity of tokens and spans
var1 = doc[2:4]
var2 = doc[5]
print(var1,'<->', var2, var1.similarity(var2))
