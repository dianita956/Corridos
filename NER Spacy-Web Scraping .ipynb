{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0476595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "\n",
    "%pip install requests -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.wkt import loads\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"tok2vec\", \"tagger\", \n",
    "                                            \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ed36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json #javascript object notation to store data outside python. watch videos from Matt Digital Humanities. \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04086688",
   "metadata": {},
   "source": [
    "## Web Scrap and Extract List of municipalities in Texas from Wiki \n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_municipalities_in_Texas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88779cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://en.wikipedia.org/wiki/List_of_municipalities_in_Texas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ac5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../corridos/listofmunicipalitiesinTexasTest.txt\"\n",
    "s = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ca1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(s.text, \"lxml\")#to tell the program to read the site as HTML (lxml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33040cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013353a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heading_extract():\n",
    "    #find a specific item from the website to make scrapping easier. \n",
    "    heading = soup.find(\"h1\",{\"id\": 'firstHeading'})\n",
    "    with open(file,\"a+\", encoding='utf-8') as f:\n",
    "             f.write(heading.get_text() + \"\\n\")\n",
    "              \n",
    "heading_extract()#check file explore\n",
    "#ensure the text file is there and inputting data from the web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_extraction():\n",
    "    table = soup.find(\"table\", class_= \"wikitable sortable\")\n",
    "    #print(table.findAll(\"tr\"))\n",
    "    with open (file, \"a+\", encoding=\"utf-8\") as f: \n",
    "        for row in table.findAll(\"tr\"):\n",
    "            #print(row)\n",
    "            for th in row.findAll(\"th\"):\n",
    "                f.write(th.get_text())\n",
    "                for td in row.findAll(\"td\"):\n",
    "                    f.write(td.get_text())\n",
    "                      \n",
    "table_extraction()\n",
    "#print(table.findAll(\"tr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104fd96",
   "metadata": {},
   "source": [
    "def do_all(a, b):\n",
    "    heading_extract(a,b)\n",
    "    table_extraction(a,b)\n",
    "\n",
    "do_all('url', 'fileName')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84adb368",
   "metadata": {},
   "source": [
    "Storing data to text files - tutorial video https://www.youtube.com/watch?v=WiWp2xqKLPo&list=PL2VXyKi-KpYtcluInhPJItpNBKIF3LO2k&index=23 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a071d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b50e1ada",
   "metadata": {},
   "source": [
    "Same code from above but in a single code box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec8cb4",
   "metadata": {},
   "source": [
    "url =\"https://en.wikipedia.org/wiki/List_of_municipalities_in_Texas\"\n",
    "\n",
    "file = \"../corridos/listofmunicipalitiesinTexas.txt\"\n",
    "s = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(s.text, \"lxml\")#to tell the program to read the site as HTML (lxml)\n",
    "\n",
    "\n",
    "#print(soup)\n",
    "\n",
    "def heading_extract():\n",
    "    #find a specific item from the website to make scrapping easier. \n",
    "    heading = soup.find(\"h1\",{\"id\": 'firstHeading'})\n",
    "   \n",
    "    with open(file,\"w+\", encoding='utf-8') as f:\n",
    "             f.write(heading.get_text()+\"\\n\")\n",
    "              \n",
    "heading_extract()\n",
    "#check file explore\n",
    "#ensure the text file is there and inputting data from the web.\n",
    "'''\n",
    "#content extraction\n",
    "def content_extract():\n",
    "    contents = soups.findAll(\"p\")\n",
    "    with open (file, \"a+\") as f:\n",
    "        for i in contents:\n",
    "            f.write(i.get_text())\n",
    "\n",
    "#sources for citation \n",
    "def get_sources():\n",
    "    sources = soup.findAll(\"cite\")\n",
    "    with open(file, \"a+\")as f:\n",
    "        f.write(\"\\nSOURCES\\n\")\n",
    "        for i in sources:\n",
    "            f.write(i.get_text())\n",
    "'''\n",
    "\n",
    "def table_extraction():\n",
    "    table = soup.find(\"table\", class_= \"wikitable sortable\")\n",
    "    \n",
    "\n",
    "    #print(table.findAll(\"tr\"))\n",
    "    with open (file, \"a+\", encoding=\"utf-8\")as f: \n",
    "        for row in table.findAll(\"tr\"):\n",
    "            headerString  = \"\"\n",
    "            dataString = \"\"\n",
    "        \n",
    "            #print(row)\n",
    "            for th in row.findAll(\"th\"):\n",
    "                headerString  = headerString+th.get_text(strip=True)+\"|\"\n",
    "                print(headerString)\n",
    "                #ending a line break to organize the txt file\n",
    "            f.write(headerString)\n",
    "            f.write(\"\\n\")\n",
    "           \n",
    "                #print(row)\n",
    "            for td in row.findAll(\"td\"):\n",
    "                dataString = dataString+td.get_text(strip=True)+\"|\"\n",
    "                print(dataString)\n",
    "                \n",
    "            f.write(dataString)\n",
    "                      \n",
    "table_extraction()\n",
    "#print(table.findAll(\"tr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38701884",
   "metadata": {},
   "source": [
    "## Text Analysis: Named Enities Reconginzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('../Corridos/corrido corpus/ElCorridodeGregorioCortez_X.txt', encoding=\"utf-8\").read()\n",
    "for number, sentence in enumerate(nltk.sent_tokenize(text)):\n",
    "    print(sentence)\n",
    " # Break text into sentences\n",
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc =nlp(text)\n",
    "\n",
    "ents =list(doc.ents)\n",
    "\n",
    "place= []\n",
    "\n",
    "for ent in ents:\n",
    "    if ent.label_ == \"GPE\":\n",
    "        place.append(ent)\n",
    "        \n",
    "print(place)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8918814",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb481a",
   "metadata": {},
   "source": [
    "Processing Text using SpaCy and pipelines for NERs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faec0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"en\"\n",
    "pipeline = [\"ner\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Greg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c33b8046986a03aa44ef4ea500a2630dcd79ff60fec54402ff1309c4dcbd3e63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
