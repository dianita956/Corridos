{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Introduction to Cultural Analytics and Python https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/Multilingual/Spanish/02-Named-Entity-Recognition-Spanish.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'thinc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\azg526\\Documents\\GitHub\\Corridos\\Spacy_NER_ES.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/azg526/Documents/GitHub/Corridos/Spacy_NER_ES.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azg526/Documents/GitHub/Corridos/Spacy_NER_ES.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m \u001b[39mimport\u001b[39;00m displacy\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azg526/Documents/GitHub/Corridos/Spacy_NER_ES.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mes\u001b[39;00m \u001b[39mimport\u001b[39;00m Spanish\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\__init__.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumpy.ufunc size changed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneural\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m prefer_gpu, require_gpu\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcli\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfo\u001b[39;00m \u001b[39mimport\u001b[39;00m info \u001b[39mas\u001b[39;00m cli_info\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'thinc'"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy import displacy\n",
    "from spacy.lang.es import Spanish\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_colwidth = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_lg\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "nlp.tokenizer.token_match = Spanish.Defaults.token_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'corrido corpus\\gregoriocortez_es_corrido.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    print(named_entity, named_entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "number_of_chunks = 80\n",
    "\n",
    "chunk_size = math.ceil(len(text) / number_of_chunks)\n",
    "\n",
    "text_chunks = []\n",
    "\n",
    "for number in range(0, len(text), chunk_size):\n",
    "    text_chunk = text[number:number+chunk_size]\n",
    "    text_chunks.append(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_documents = list(nlp.pipe(text_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = []\n",
    "\n",
    "for document in chunked_documents:\n",
    "    for named_entity in document.ents:\n",
    "        if named_entity.label_ == \"PER\":\n",
    "            people.append(named_entity.text)\n",
    "\n",
    "people_tally = Counter(people)\n",
    "\n",
    "PERdf = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
    "PERdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Places/LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "for document in chunked_documents:\n",
    "    for named_entity in document.ents:\n",
    "        if named_entity.label_ == \"LOC\":\n",
    "            places.append(named_entity.text)\n",
    "\n",
    "places_tally = Counter(places)\n",
    "\n",
    "LOCdf = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
    "LOCdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get NER in Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def get_ner_in_context(keyword, document, desired_ner_labels= False):\n",
    "    \n",
    "    if desired_ner_labels != False:\n",
    "        desired_ner_labels = desired_ner_labels\n",
    "    else:\n",
    "        desired_ner_labels = ['PER', 'ORG', 'LOC']  \n",
    "        \n",
    "    #Iterate through all the sentences in the document and pull out the text of each sentence\n",
    "    for sentence in document.sents:\n",
    "        #process each sentence\n",
    "        sentence_doc = nlp(sentence.text)\n",
    "        for named_entity in sentence_doc.ents:\n",
    "            #Check to see if the keyword is in the sentence (and ignore capitalization by making both lowercase)\n",
    "            if keyword.lower() in named_entity.text.lower()  and named_entity.label_ in desired_ner_labels:\n",
    "                #Use the regex library to replace linebreaks and to make the keyword bolded, again ignoring capitalization\n",
    "                #sentence_text = sentence.text\n",
    "            \n",
    "                sentence_text = re.sub('\\n', ' ', sentence.text)\n",
    "                sentence_text = re.sub(f\"{named_entity.text}\", f\"**{named_entity.text}**\", sentence_text, flags=re.IGNORECASE)\n",
    "\n",
    "                display(Markdown('---'))\n",
    "                display(Markdown(f\"**{named_entity.label_}**\"))\n",
    "                display(Markdown(sentence_text))\n",
    "\n",
    "for document in chunked_documents:\n",
    "    get_ner_in_context('Gonzales', document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERdf.columns = PERdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "PERdf.columns\n",
    "PERdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERdf.head(15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2733bea9471b811f11fcc270fedd0cbc3ce9aeda02cc5b16cc33d15a4e71cb16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
